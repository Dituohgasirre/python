scrapy爬虫框架
    scrapy是一个用Python写的免费且开源的网络爬虫框架
    scrapy起初被设计用来做网络抓取(scraping)，现也可做通用的网络爬取(crawling)
    scrapy爬取网页，并抽取出结构化的数据，这些数据可应用于许多场合，\
    比如说数据挖掘、信息处理、历史记录归档等
    一个用scrapy构建的爬虫项目是围绕蜘蛛(spider)来构建的，scrapy的蜘蛛\
    是一个Python类，它包含了爬虫工作的具体逻辑
    scrapy提供了一个shell工具，可用于开发者测试其对网站行为的推测

安装scrapy
    使用pip工具可以方便地安装scrapy
        pip install scrapy

scrapy项目
    创建项目，假设项目名字为tutorial
        scrapy startproject tutorial

scrapy项目的目录结构
    tutorial
    ├── scrapy.cfg
    └── tutorial            <-- 项目的包
        ├── __init__.py
        ├── items.py        <-- 项目的“项”的定义
        ├── middlewares.py  <-- 项目的“中间件”定义
        ├── pipelines.py    <-- 项目的“管道”定义
        ├── settings.py     <-- 项目的设定
        └── spiders         <-- “蜘蛛”存放的目录
            └── __init__.py

一个简单的Spider
    import scrapy
    class QuotesSpider(scrapy.Spider):
        name = "quotes"
        def start_requests(self):
            urls = [
                'http://quotes.toscrape.com/page/1/',
                'http://quotes.toscrape.com/page/2/',
            ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

一个简单的Spider（续）
    def parse(self, response):
        pagenum = response.url.rstrip('/').split('/')[-1]
        filename = 'quotes-%s.html' % pagenum
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)

一个简单的Spider（续）
    要点
        spider类必须继承scrapy.Spider
        name 是项目中的spider的唯一标识
        start_requests方法必须返回一个可迭代对象
        parse方法用于处理抓取的结果

运行爬虫
    继上例，运行项目tutorial中的quotes蜘蛛的shell命令如下，这里的quotes\
    是爬虫在项目中独一的名字
        scrapy crawl quotes
    命令执行完毕后，当前目录应该会产生两个quotes-X.html的文件
    如果蜘蛛所在的模块可以脱离爬虫项目运行，则也可以用以下方法运行
        scrapy runspider spider-file-name.py

一个基本的爬虫所需的部件
    继承scrapy.Spider类
    必须提供一个name的类属性，此值必须是项目中唯一
    提供一个start_requests方法，此方法返回一个可迭代对象，每一个元素就是一个scrapy.Request
    提供一个parse方法，此方法返回一个可迭代对象，每一个元素就是一个scrapy.Response

Spider类中的常用属性和方法
    name
    allowed_domain
    start_urls
    custom_settings
    crawler
    settings
    logger
    start_requests()
    parse()
    log()

省略start_requests方法
    start_requests方法的作用是生成一系列的scrapy.Request对象
    scrapy默认的start_requests已经实现了基本的功能，默认会调用parse方法
    只需要定义一个类属性start_urls即可，继上例，去掉start_requests方法，\
    添加start_urls属性
    class QuotesSpider(scrapy.Spider):
        name = "quotes"
        start_urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]

抽取网页中的数据
    scrapy下载到网页后，调用spider中的parse方法，解析网页
    解析网页所得的信息可保存做后期使用，其中的URL根据需要可做进一步的爬取
    抽取网页中的数据，关键在于定位数据，scrapy提供了不同的定位抽取方法

交互式地测试scrapy的选择器
    在交互式环境中测试scrapy的选择器有助于快速地抽取网页的内容
    使用scrapy的shell命令做交互式抽取
        scrapy shell "http://quotes.toscrape.com/page/1/"
    交互式环境下操控scrapy
        可用的scrapy相关对象
            scrapy      <-- scrapy模块
            crawler
            item
            request     <-- 请求对象
            response    <-- 响应对象
            settings    <-- scrapy的设定对象
            spider
        便捷的函数
            fetch(url[, redirect=True])   <-- 获取url的内容，更新相应的对象
            fetch(req)                    <-- 获取一个scrapy.Request，更新相应的对象
            shelp()                       <-- 显示帮助信息
            view(response)                <-- 用浏览器浏览响应对象

抽取网页中的数据
    在scrapy中，可以使用XPath和CSS来选择网页元素，scrapy的选择器是基于XPath的
    选择
        response.xpath('//title')
        response.xpath('//p/text()')
        response.xpath('//a/@href')
    抽取
        response.xpath('//p/text()').extract()
        response.xpath('//p/text()').extract_first()
        response.xpath('//p/text()')[0].extract()
        response.xpath('//p/text()').re(r'(\w+)')
        response.xpath('//p/text()').re_first(r'(\w+)')
    可以用浏览器打开response对象，用其中的调试工具协助定位网页元素

抽取网页中的数据（续）
    scrapy的Selector对象可以做进一步的选取
        quote = response.xpath('//div[@class="quote"]')[0]
        quote.xpath('span[1]/text()').extract_first()
    抽取 'http://quotes.toscrape.com/page/1/' 的信息范例
        for quote in response.xpath('//div[@class="quote"]'):
            text = quote.xpath('span[1]/text()').extract_first()
            author = quote.xpath('.//small/text()').extract_first()
            tags = quote.xpath('div[@class="tags"]/a[@class="tag"]/text()').extract()
            print(dict(text=text, author=author, tags=tags))

保存抓取到的数据
    抓取到的数据，就是parse方法返回的数据，可以存放到文件中
        scrapy crawl quotes -o quotes.json
        scrapy crawl quotes -o quotes.jl
        scrapy crawl quotes -o quotes.csv
        scrapy crawl quotes -o quotes.xml
    如果需要对抓取到的数据做更多的复杂操作，可以使用Item Pipeline

scrapy的Item
    从网页上抓取的数据是结构化的数据，Item可用于方便地组织这些结构化的数据
    定义方法
        class Quote(scrapy.Item):
            text = scrapy.Field()
            author = scrapy.Field()
            tags = scrapy.Field()
    定义Item时指定的scrapy.Field实例不会成为Item类的属性，可以通过Item.fields来访问
    Item的Field (scrapy.Field) 实质上是dict的一个子类
    Item的Field中可以包含任意数量的任意key，作为metadata

操作Item的实例
    操作Item的实例类似于操作字典实例
    操作范例
        quote = Quote(text='aa', author='alice', tags=['good', 'beauti'])
        quote['text']
        quote.get('text')
        'text' in quote         <-- 是否有数据
        'text' in quote.fields  <-- 是否定义
        quote['text'] = 'new value'
        quote.keys()
        quote.values()
        quote.items()

使用Item Loader来给Item填充数据
    Item Loader提供了一种结构化的数据填充方式
    利用Item Loader可以给Item的每个字段指定输入处理器和输出处理器
    输入和输出处理器都是可调用对象，它接收唯一的可迭代对象，并返回处理结果
    输入处理器的结果暂存在Item Loader内部的一个列表中，并没有赋给Item的字段
    多次给同一个字段添加值时，输入处理器的结果被附加在Item Loader内部的一个列表中
    输出处理器的结果赋给Item字段
    可以通过add_xpath, add_css, add_value三种方法给Item字段添加值

Item Loader范例
    class GoodsItemLoader(ItemLoader):
        default_item_class = GoodsItem
        default_output_processor = TakeFirst()
        price_in = MapCompose(float)
        item_name_in = MapCompose(str.strip)
        shop_name_in = MapCompose(str.strip)

Item Loader范例（续）
    class GoodsItem(scrapy.Item):
        item_name = scrapy.Field(input_processor=MapCompose(str.strip))
        item_url = scrapy.Field()
        price = scrapy.Field(input_processor=MapCompose(float))
        turnover = scrapy.Field()
        comments = scrapy.Field()
        shop_name = scrapy.Field(input_processor=MapCompose(str.strip))
        shop_url = scrapy.Field()

Item Loader范例（续）
    divs = response.css('#result_list > div')
    for div in divs:
        l = GoodsItemLoader(selector=div, response=response)
        l.add_css('price', '.productPrice > em::attr(title)')
        l.add_css('shop_name', '.productShop > a::text')
        l.add_xpath('turnover', './/p[@class="productStatus"]//em/text()')
        l.add_xpath('comments', './/p[@class="productStatus"]//a/text()')
        yield l.load_item()

输入输出处理器的优先级
    从高到低：
        Item Loader定义中的field_in/field_out
        Item定义中的字段metadata中的input_processor/output_process
        Item Loader定义中的default_input_processor/default_output_processor

给处理器传递上下文信息
    当需要给处理器传输额外的信息时，可以使用Loader的context
    当函数接收一个名叫loader_context的关键字参数时，Item Loader就会给它传递上下文
        def urljoin(url, loader_context):
    可以在这些地方提供上下文信息：
        实例化Loader时：loader = ItemLoader(product, unit='cm')
        修改已有loader：loader.context['unit'] = 'cm'
        定义Loader时：length_out = MapCompose(parse_length, unit='cm')

Item管道
    当蜘蛛抓取了一个Item后，这个Item会被发送到一系列的Item管道中，被依次处理
    每一个Item管道都是一个Python类，这个类接收一个Item，对它做处理，然后决定\
    让这个Item继续被送往下一个管道或者被丢弃
    Item管道的用途
        清理HTML数据
        校验数据
        检查重复的Item
        把Item存储到数据库中

书写Item管道
    Item管道这个类至少要实现方法process_item(self, item, spider)，此方法必须\
    返回一个Item，或者一个字典，一个Twisted Deferred，或者抛出scrapy.DropItem\
    异常，被Drop掉的Item不会再被后续的管道处理
    以下方法是可选方法
        open_spider(self, spider)
        close_spider(self, spider)
        from_crawler(cls, crawler)
    from_crawler方法必须返回一个Item管道实例，此方法应被实现为“类方法”，它可\
    被用来通过crawler中的信息创建一个Item管道实例，crawler对象中含有爬虫\
    的各种配置信息

Item管道范例
    from scrapy.exceptions import DropItem
    class ValidatePipeline:
        def process_item(self, item, spider):
            if 'price' not in item:
                raise DropItem("Missing price in %s" % item)    
            return item

Item管道范例（续）
    import json
    class JsonWriterPipeline:
        def open_spider(self, spider):
            self.file = open('items.jl', 'w')
        def close_spider(self, spider):
            self.file.close()
        def process_item(self, item, spider):
            line = json.dumps(dict(item)) + "\n"
            self.file.write(line)
            return item

Item管道范例（续）
    import MySQLdb
    class MysqlPipeline:
        tbname = 'quotes'
        def __init__(self, user, passwd, db,
                     host='localhost', port=3306, charset='utf8'):
            self.user = user
            self.passwd = passwd
            self.db = db
            self.host = host
            self.port = port
            self.charset = charset
        @classmethod
        def from_crawler(cls, crawler):
            return cls(
                user=crawler.settings.get('DB_USER'),
                passwd=crawler.settings.get('DB_PASSWD'),
                db=crawler.settings.get('DB_DB'),
                host=crawler.settings.get('DB_HOST'),
                port=crawler.settings.get('DB_PORT'),
                charset=crawler.settings.get('DB_CHARSET')
            )
        def open_spider(self, spider):
            self.conn = MySQLdb.connect(host=self.host, port=self.port,
                                        user=self.user, passwd=self.passwd,
                                        database=self.db, charset=self.charset)
            self.cursor = self.conn.cursor()
        def close_spider(self, spider):
            self.conn.commit()
            self.conn.close()
        def process_item(self, item, spider):
            sql = 'insert into %s ' % self.tbname
            sql += 'values (%s, %s, %s)'
            data = (item['text'], item['author'], item['tags'])
            self.cursor.execute(sql, args=data)
            self.conn.commit()
            return item


启用Item管道
    Item管道定义好后，需要在项目的设定模块settings.py中启用
    代码
        ITEM_PIPELINES = {
            'myproject.pipelines.PricePipeline': 300,
            'myproject.pipelines.JsonWriterPipeline': 800,
        }

通过ImagesPipeline下载图片
    scrapy提供了一个ImagesPipeline用于方便地下载图片
    ImagesPipeline下载的图片会被自动转换成JPG
    ImagesPipeline不会重复下载同一个图片
    item的image_urls中的url将会被当作图片下载
    下载的结果存放在名叫images的键下

使用ImagesPipeline下载图片
    配置 ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}
    配置 IMAGES_STORE = '/path/to/valid/dir'
    在item中给image_urls键设置图片的地址，如果使用Item类，可以定义这样的字段
        class MyItem(scrapy.Item):
            ...
            image_urls = scrapy.Field()
            images = scrapy.Field()


Request/Response的产生及相关的处理组件
    一般情况下，Request由Spider生成
    一般情况下，Request由Downloader处理
    一般情况下，Downloader处理Request后生成Response


Request的常用参数
    url         <-- 请求的目标
    callback    <-- 处理request成功时的处理函数
    meta        <-- 初始的meta数据，可在response.meta 中访问
    headers     <-- 类字典对象，包含了请求的头部信息
    cookies     <-- 传给服务器的cookie
    dont_filter <-- 使request不被去重器率掉，小心使用
    errback     <-- 处理request遇到异常时的处理函数


通过request.meta可以给回调函数传递额外的数据
    def parse_page1(self, response):
        item = MyItem()
        item['main_url'] = response.url
        request = scrapy.Request("http://www.example.com/some_page.html",
                                 callback=self.parse_page2)
        request.meta['item'] = item
        yield request
    def parse_page2(self, response):
        item = response.meta['item']
        item['other_url'] = response.url
        yield item


使用FormRequest来做登录操作
    FormRequest是Request的子类，通过POST请求给服务器发送数据
    处理FormRequest请求成功后，服务器返回的cookie可被后续操作使用
    范例：
        yield FormRequest(
            url="http://www.example.com/post/action",
            formdata={'name': 'John Smith', 'age': '18'},
            callback=self.after_post
        )

使用FormRequest.from_response来登录
    class LoginSpider(scrapy.Spider):
        name = 'example.com'
        start_urls = ['http://www.example.com/users/login.php']
        def parse(self, response):
            return scrapy.FormRequest.from_response(
                response,
                formdata={'username': 'john', 'password': 'secret'},
                callback=self.after_login
            )
        def after_login(self, response):
            if "authentication failed" in response.body:
                self.logger.error("Login failed")
                return

Response的常用参数
    url         <-- 产生这个回应的地址
    status      <-- 回应的http状态，默认为200
    headers     <-- 类字典对象，包含了回应的头部信息
    body        <-- 一个字节串(bytes)，可从response.text中得到解码后的值
    request     <-- 代表生成这个回应的请求

Response常用属性和方法
    meta        <-- 指向response.request.meta
    copy        <-- 复制response
    replace     <-- 复制response，并替换指定的属性
    urljoin     <-- 根据response的url生成绝对路径url
    follow      <-- 生成一个Request对象用来跟踪链接


Spider中间件
    Spider中间件的作用
        处理输入给Spider的response
        处理Spider生成的item和Request

启用Spider中间件
    SPIDER_MIDDLEWARES = {
        'myproject.middlewares.CustomSpiderMiddleware': 543,
    }
    SPIDER_MIDDLEWARES中的设定将会与SPIDER_MIDDLEWARES_BASE设定合并
    合并的结果按照字典中的value的值排序
    数字越小的中间件越靠近engine，越大越靠近Spider
    process_spider_input方法按数字从小到大执行（顺序）
    process_spider_output方法按数字从大到小执行（反序）
    如果需要禁用某个内置的中间件，可以设置其顺序为None
        SPIDER_MIDDLEWARES = {
            'myproject.middlewares.CustomSpiderMiddleware': 543,
            'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': None,
        }

SPIDER_MIDDLEWARES_BASE
    'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50,
    'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': 500,
    'scrapy.spidermiddlewares.referer.RefererMiddleware': 700,
    'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware': 800,
    'scrapy.spidermiddlewares.depth.DepthMiddleware': 900,

Downloader中间件
    Downloader中间件的作用
        处理Request
        处理Response

启用Downloader中间件
    启用Downloader中间件，方法与Spider中间件类似
        DOWNLOADER_MIDDLEWARES = {
            'myproject.middlewares.CustomDownloaderMiddleware': 543,
        }
    DOWNLOADER_MIDDLEWARES中的设定将会与DOWNLOADER_MIDDLEWARES_BASE设定合并
    合并的结果按照字典中的value的值排序
    数字越小的中间件越靠近engine，越大越靠近Downloader
    process_request方法按数字从小到大执行（顺序）
    process_response方法按数字从大到小执行（反序）
    如果需要禁用某个内置的中间件，可以设置其顺序为None
        DOWNLOADER_MIDDLEWARES = {
            'myproject.middlewares.CustomDownloaderMiddleware': 543,
            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
        }

SPIDER_MIDDLEWARES_BASE
    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,
    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,
    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,
    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,
    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,
    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,
    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,
    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,
    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,
    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,

设置下载延迟
    如果下载请求太密集的话，可能会给服务器造成太大负担，也有可能被封锁
    可以通过在settings.py中设置DOWNLOAD_DELAY来指定一个下载间隔
    DOWNLOAD_DELAY受RANDOMIZE_DOWNLOAD_DELAY影响，实际下载间隔是：
        0.5 * DOWNLOAD_DELAY 到 1.5 * DOWNLOAD_DELAY 之间

自动流控(Auto throttle)
    自动流控基于爬虫所在的系统和被抓取的服务器的负载来调整爬取速度
    自动流控的设计目标
        使得爬虫对被抓取的服务器更“友好”
        自动调整抓取速度，使用户无需为了找到最佳延迟而手动测试
        用户只需设定一个最大并发请求数，自动流控就完成剩下的工作了

自动流控基本原理
    等待时间(latency)指的是从建立TCP连接到收到HTTP回应头之间的时间间隔
    自动流控通过等待时间(latency) 来计算两次下载之间的时间间隔(delay)
    如果等待时间是4秒，1个并发时，每4秒发一次请求即可
    如果等待时间是4秒，2个并发时，每2秒需要发送一次请求
    可得计算公式：等待时间latency / 并发数N = 间隔时间delay
    自动流控仍会考虑标准的scrapy并发和间隔设定，流控的结果不会超过以下三个参数的设定值
        CONCURRENT_REQUESTS_PER_DOMAIN
        CONCURRENT_REQUESTS_PER_IP
        DOWNLOAD_DELAY

常规流控设定与自动流控对比分析
    常规流控的常见设定策略
        设置一个相对较小的DOWNLOAD_DELAY
        设置CONCURRENT_REQUESTS_PER_DOMAIN 和 CONCURRENT_REQUESTS_PER_IP 
    常规流控这样设定的缺点
        这样的设定会造成偶尔的并发数暴增，因为DOWNLOAD_DELAY较小
        非200的回应通常比200的回应的返回速度更快，使得出错时并发数变大
    自动流控不存在这样的问题

自动流控的相关配置项
    AUTOTHROTTLE_ENABLED            <-- 启用自动流控
    AUTOTHROTTLE_START_DELAY        <-- 初始间隔时间
    AUTOTHROTTLE_MAX_DELAY          <-- 最大间隔时间（为了在高等待时使用）
    AUTOTHROTTLE_TARGET_CONCURRENCY <-- 自动流控的并发目标，建议值，非硬限制
    AUTOTHROTTLE_DEBUG              <-- 输出自动流控的调试信息
    CONCURRENT_REQUESTS_PER_DOMAIN  <-- 每域名最大并发数
    CONCURRENT_REQUESTS_PER_IP      <-- 每IP最大并发数，覆盖每域名的设定
    DOWNLOAD_DELAY                  <-- 两次下载之间的间隔

修改User-Agent
    网站可能会通过限制User-Agent的方法来限制爬虫
    爬虫的User-Agent标识可以人为修改
    先准备一些User-Agent数据，下例的agents是一个包含agent标识的列表
    from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
    class RandomUserAgentMiddleware(UserAgentMiddleware):
        def process_request(self, request, spider):
            agent = random.choice(agents)
            request.headers["User-Agent"] = agent

使用代理来下载网页
    当网站对爬虫的抓取频率做了限制时，可以考虑用代理来突破限制
    通过下载中间件可以设置通过代理来下载网页
    代理
        class ProxyMiddleware(object):
            def process_request(self, request, spider):
                request.meta['proxy'] = "http://PROXY_IP:PORT"
    把中间件加到settings.py文件的DOWNLOADER_MIDDLEWARES中，设置适当的顺序



通过Splash来解析JS页面

对于需要运行网页中的JavaScript 代码才能有正确的html 代码的网页，可以借助Splash 工具。
Splash 是一个浏览器，基於twisted, PyQt5，它提供了http 的API，可以通过此API 把需要解析
的url 传过去，Splash 节完之后会把html/json 代码返回给回调函数。
为了便於使用，可以通过docker 安装Splash，并使用scrapy-splash 模块。

1. 安装Splash 和scrapy-splash 模块，运行Splash 服务器，
   scrapy 的版本要满足scrapy-splash 的要求：

$ docker pull scrapinghub/splash
$ docker run -p 8050:8050 scrapinghub/splash
$ pip install scrapy-splash

2. 在蜘蛛项目的settings.py 中添加以下配置：

SPLASH_URL = 'http://localhost:8050'
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
SPIDER_MIDDLEWARES = {
    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
}
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

3. 修改蜘蛛代码，给蜘蛛类增加一个方法start_requests：

from scrapy_splash import SplashRequest

    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, args={'wait': 0.5})

安装selenium/Firefox/PhantomJS
    安装firefox/PhantomJS
        apt install firefox
        apt install phantomjs
    安装firefox的控制工具
        https://github.com/mozilla/geckodriver/releases
        从此页面下载对应操作系统的版本，解压，把其中的程序文件
        放到系统的 /usr/local/bin 里面，确保程序文件有执行权限
    安装selenium
        pip3 install selenium
    selenium文档
        http://www.seleniumhq.org/docs/03_webdriver.jsp

selenium+Firefox基本用法
    导入模块
        from selenium import webdriver
    启动/关闭Firefox浏览器
        d = webdriver.Firefox() <-- 启动浏览器
        d.quit()                <-- 关闭浏览器
    打开网页
        d.get('http://www.gnu.org')
    定位网页中的元素
        d.find_element_byXXX
        d.find_elements_byXXX
    在输入框中输入数据
        element = d.find_element_by_id('id_of_element')
        element.send_keys('content')
    点击按钮
        element.click()

通过PhantomJS/Firefox来下载网页
    可以通过下载中间件来把下载请求转给另外的程序处理，然后返回结果
    此下载中间件需要返回一个Response对象
        class FirefoxMiddleware:
            def process_request(self, request, spider):
                text = b'<html><body>hello, world.</body></html>'
                response = HtmlResponse(url='http://abc.com', status=200,
                                        body=text)
                return response




从多个相关的网页获取一个item的信息，参考代码：

import scrapy


class StackSpider(scrapy.Spider):
    name = 'stack'
    start_urls = [
        'https://stackoverflow.com/?tab=month',
    ]

    def parse(self, response):
        divs = response.xpath('//div[@id="qlist-wrapper"]/div[1]/div[1]/div')
        for div in divs:
            title = div.xpath('.//h3/a/text()').extract_first()
            url = div.xpath('.//h3/a/@href').extract_first()
            url = response.urljoin(url)
            tags = div.xpath('.//div[@class="summary"]/div[1]/a/text()').extract()
            item = dict(title=title, url=url, tags=tags)
            request = scrapy.Request(url=url,
                                     callback=self.determine_status,
                                     meta={'item': item})
            yield request

    def determine_status(self, response):
        accepted = bool(response.xpath('//span[text()="accepted"]'))
        item = response.request.meta['item']
        item['accepted'] = accepted
        return item




自动跟随并爬取网页中的超链接，参考代码：

import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    allowed_domains = ["quotes.toscrape.com"]
    start_urls = [
        'http://quotes.toscrape.com',
    ]

    def parse(self, response):
        # yield the items
        divs = response.xpath('//div[@class="quote"]')
        for div in divs:
            text = div.xpath('span[@class="text"]/text()').extract_first()
            author = div.xpath('.//small/text()').extract_first()
            tags = div.xpath('div[@class="tags"]/a/text()').extract()
            yield dict(text=text, author=author, tags=tags)

        # follow links
        for url in response.xpath('//a/@href').extract():
            url = response.urljoin(url)
            yield scrapy.Request(url=url, callback=self.parse)



分布式的scrapy，共用相同的下载队列 (许多的request)，多个scrapy实例从同一个队列中获取request，同时下载。scrapy-redis 提供了一系列的组件，用来代替scrapy默认的组件，进行统一调度，统一判重，以达到分布式爬取的功能。



使用JOBDIR来暂停/继续爬虫的工作
    指定一个用来存放状态数据的目录即可
        scrapy crawl quotes-loader -s JOBDIR=quotes_job
    不同爬虫之间不能共用同一个目录
    相同爬虫不同次任务之间不能共用同一个目录

通过scrapyd部署爬虫程序
    scrapyd可以管理多个项目，每个项目可以有多个版本，但只有最新版本起作用
    scrapyd进程常驻后台运行，监听请求，启动新进程来运行爬虫
    用scrapyd运行爬虫时相当于执行这个命令
        scrapy crawl spider
    scrapyd通过并发的形式执行多进程。
    scrapyd提供了JSON web服务API用来上传项目和调度爬虫
    scrapyd提供了网页界面用来监控进程和查看日志，地址是
        http://localhost:6800/

使用scrapyd
    安装
        pip install scrapyd
    运行
        scrapyd
    查看状态
        http://localhost:6800/

scrapyd-client的使用
    scrapyd-client工具可以简化对scrapyd的操作
    安装scrapyd-client
        下载地址：https://github.com/scrapy/scrapyd-client
    scrapyd-client命令在项目根目录下运行
    发布名叫quotes的项目
        scrapyd-client deploy -p quotes
    列出scrapyd上的项目
        scrapyd-client projects
    列出scrapyd上项目quotes里的spider
        scrapyd-client spiders -p quotes
    运行（调度）spider
        scrapyd-client schedule -p quotes spider
