一、写一个程序，要求：

1. 通过selenium在百度搜索关键字 "python"
2. 抽取前面10个结果的这些数据：网页的url，网页的标题
   抽取内容可以考虑这些工具：BeautifulSoup, lxml
3. 把结果存储在json格式的文件中


二、在spider中使用Item

1. 在items.py模块中定义Item，Item必须继承scrapy.Item
    class XXItem(scrapy.Item):
        a = scrapy.Field()
        b = scrapy.Field()
2. 在spider的模块中使用Item
    item = XXItem()
    item['a'] = a
    item['b'] = b
    yield item


三、通过Item管道去除重复的Item

要点：
    1. 定义一个管道类，定义process_item 方法
        process_item(self, item, spider)
    2. 在方法 open_spider 中用一个列表来存储已经出现过的item
        open_spider(self, spider)
    3. 可以考虑用hashlib里面的方法生成唯一的表示，作为item的标识
    4. 如果item的标识已经存在过，就抛出scrapy.DropItem异常
    5. 如果item还没有出现过，就要return此item对象
    6. 在settings模块中启用管道，配置项是 ITEM_PIPELINES



四、通过Item管道去除无效的Item

要点：
    1. 通过检测item所有域是否都有值，而且值都不为None，来确认item是否有效
    2. 可以把线上的网页拿下来，运行本地的web服务器，这样可以模拟无效数据，
       在网页所在的目录运行此脚本：python3 simple_server.py

参考步骤：

1. 在模块 pipelines.py 中，添加管道（一个类）

    class QuotesValidationPipeline:
        def process_item(self, item, spider):
            for field in item.fields:
                if field not in item or item[field] is None:
                    raise DropItem('field %s is required for item' % field)
            return item

2. 启用管道

    在settings.py 模块中，用类似以下的代码启用管道，右边的数字是管道的顺序号：

        ITEM_PIPELINES = {
            'quotes.pipelines.QuotesValidationPipeline': 100,
        }



五、把Item存放到自定义格式的文件中

参考代码：
    class TextWriterPipeline:

        def open_spider(self, spider):
            self.file = open('items.txt', 'w')
        def close_spider(self, spider):
            self.file.close()
        def process_item(self, item, spider):
            for k in ['text', 'author', 'tags']:
                value = item[k]
                self.file.write('%s: %s\n' % (k, value))
            self.file.write('\n')
            return item


六、把Item存放到自定义格式的文件中，每一个item都存放到独立的文件中

参考代码：
    import hashlib
    class TextWriterPipeline:
        def process_item(self, item, spider):
            fmt = 'text: %(text)s\nauthor: %(author)s\ntags: %(tags)s\n'
            content = fmt % item
            fname = hashlib.sha1(content.encode()).hexdigest() + '.txt'
            with open(fname, 'w') as f:
                f.write(content)
            return item



七、把Item存放到MySQL数据库中

1. 安装MySQL服务器
    apt install mysql-server
2. 安装Python的mysql库
    pip install mysqlclient
3. 在Python程序中调用
    import MySQLdb as db
4. 启动数据库服务器
    /etc/init.d/mysql start
5. 预备数据库、数据表、用户、权限
    mysql
    create database scrapy default charset utf8;
    use scrapy
    create table quotes (text text, author varchar(128), tags varchar(256));
    grant all on scrapy.* to scrapy@localhost identified by 'scrapy';
6. 从ipython中连接数据库
    import MySQLdb as db
    conn = db.connect(host='localhost', port=3306,
                      user='scrapy', passwd='scrapy',
                      database='scrapy', charset='utf8')
7. 操纵数据
    cur = conn.cursor()
    sql = 'insert into quotes values("Good", "John Smith", "deepthought,good")'
    cur.execute(sql)
    conn.commit()
8. 创建Item管道，提供__init__, open_spider, close_spider, from_crawler, process_item 等方法，并预防SQL注入，把连接数据库的信息存放到配置模块中



==============


一、写一个scrapy爬虫，下载一个拉勾的职位信息网页

1. 网页url: https://www.lagou.com/jobs/2666427.html



二、给scrapy的项目加上随机User-Agent的功能

通过下载器中间件来实现：

1. 修改项目目录下的 middlewares.py 模块，添加以下中间件定义：

    from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
    class RandomUserAgentMiddleware(UserAgentMiddleware):
        def process_request(self, request, spider):
            agent = random.choice(agents)
            request.headers["User-Agent"] = agent

2. 在settings.py 模块中启用中间件，注意顺序

    DOWNLOADER_MIDDLEWARES = {
        'tutorial.middlewares.RandomUserAgentMiddleware': 543,
    }


三、使用随机User Agent 库

1. 安装
pip install scrapy-fake-useragent

2. 启用
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,
}


================


一、按以下要求爬取网页

要求：
    1. 爬取 'https://stackoverflow.com/?tab=month'
    2. 获取右列信息：标题，url，标签，是否已有被接受的答复
    3. 存放到 stack.jl 文件中




二、按以下要求爬取网页

要求：
    1. 从唯一的url 'http://quotes.toscrape.com' 开始爬取
    2. 获取右列信息：text, author, tags
    3. 存放到 quotes.jl 文件中
    4. 跟随网页中所有的超链接，继续爬取
    5. 忽略已经爬取过的链接
    6. 仅爬取域名 quotes.toscrape.com 下的网页



三、搭建一个大型的分布式爬虫系统

要求：
    1. 使用scrapy-redis来实现
    2. 爬quotes.toscrape.com上所有的quotes
    3. 分布式系统包含N个节点
    4. 爬取的item存放到mysql数据库

1. 安装中央服务器上的Redis, MySQL数据库
    apt install redis-server
    apt install mysql-server
2. 设置redis服务器
    修改配置文件 /etc/redis/redis.conf ，注释其中的bind语句
3. 运行redis服务器
    redis-server /etc/redis/redis.conf
4. 运行mysql服务器，创建数据库/数据表/用户，并授权
    /etc/init.d/mysql start
    mysql
    create database scrapy default charset utf8;
    use scrapy
    create table quotes (text text, author varchar(128), tags varchar(256));
    grant all on scrapy.* to scrapy@localhost identified by 'scrapy';
5. 在爬虫的工作节点上安装以下组件
    pip install scrapy-redis
    pip install redis
6. 创建爬虫项目
    scrapy startproject dist_quotes
7. 按照scrapy-redis模块的需要配置项目, settings.py，可以参考scrapy-redis官方文档中的示范项目的相应文件
8. 写Item管道，用来把Item数据存放到mysql数据库中
9. 创建spider类
10. 运行各个节点上的spider
11. 在redis中给爬虫添加起始url
    rpush quotes:start_urls "http://quotes.toscrape.com"



2018-01-25 当堂练习

一、在本地用Django创建一个网站，并运行，然后测试selenium的功能

    1. 自动登录
    2. 自动给模型添加新的记录


二、按以下要求抓取信息

1. 承接上一个练习
2. 首先在后台手动添加几条记录
3. 然后用selenium来自动把记录的以下信息抓取下来
    1. 名字
    2. 年龄
    3. 创建时间
4. 存放到Json Lines文件中




2018-01-26 当堂练习

一、写一个简单的爬虫，要求如下

1. 下载页面 http://quotes.toscrape.com/
2. 不对内容做任何解析，只是简单地存储html数据到本地
3. 网页源码在 response.text 里面

爬虫的基本部件：

    继承scrapy.Spider类
    必须提供一个name的类属性，此值必须是项目中唯一
    提供一个start_requests方法，此方法返回一个可迭代对象，每一个元素就是一个scrapy.Request
    提供一个parse方法，此方法返回一个可迭代对象，每一个元素就是一个scrapy.Response


二、写一个Item类，把原本简单存放到dict中的数据存放到Item中

参考步骤：

1. 在 items.py 中定义一个scrapy.Item的子类

    class QuotesItem(scrapy.Item):
    text = scrapy.Field()
    author = scrapy.Field()
    tags = scrapy.Field()

2. 在 spider 源文件中使用定义好的 Item 类

    from quotes.items import QuotesItem

    ...
    ...

        item = QuotesItem()
        item['text'] = text
        item['author'] = author
        item['tags'] = tags
        yield item


三、改写之前的quotes爬虫，写一个ItemLoader类，用来加载item的数据

参考步骤

1. 在模块 items.py 中定义一个Item Loader

    from scrapy.loader import ItemLoader
    from scrapy.loader.processors import MapCompose, TakeFirst

    class QuotesItemLoader(ItemLoader):
        default_input_processor = MapCompose(str.strip)
        default_output_processor = TakeFirst()

2. 在 spider 的模块中使用Item Loader

    from quotes.items import QuotesItem, QuotesItemLoader

    def parse(self, response):
        divs = response.xpath('//div[@class="quote"]')
        for div in divs:
            l = QuotesItemLoader(QuotesItem(), selector=div)
            l.add_xpath('text', './/span[@class="text"]/text()')
            l.add_xpath('author', './/*[@itemprop="author"]/text()')
            l.add_xpath('tags', './/*[@itemprop="keywords"]/@content')
            item = l.load_item()
            yield item


四、写一个爬虫，要求如下：

1. 下载 http://books.toscrape.com/catalogue/page-1.html 上的书的以下信息

    1. title
    2. price
    3. image


五、写一个爬虫，要求如下：

1. 从页面 https://gitee.com/explore/recommend 获取每一个项目的以下信息：

    1. 项目作者
    2. 作者加入码云的时间


六、使用FormRequest在scrapy中登录

参考信息

    1. 运行Django项目服务器
        python manage.py runserver
    2. 用这些信息登录：{'username': 'kyo', 'password': 'abc'}
        登录所有的url：http://localhost:8000/polls/login/
    3. 登录成功后，应能看到主页 http://localhost:8000/polls/ 的内容
    4. 如何判断登录是否成功？
        判断页面源码是否包含 'login failed' 字符串

参考代码

    from scrapy import FormRequest
    class DjangoSpider(scrapy.Spider):
        name = 'django'
        def start_requests(self):
            login_url = 'http://localhost:8000/polls/login/'
            yield FormRequest(
                url=login_url,
                formdata={'username': 'kyo', 'password': 'abc'},
                callback=self.after_post
            )
        def after_post(self, response):
            if 'login failed' not in response.text:
                self.log('login successful')


七、用 PhantomJS+selenium+scrapy 实现深入爬取

参考步骤

# 1. 在一台主机(js-server)上运行PhantomJS进程，此进程常驻后台
# 2. 在js-server上运行一个Django 网页服务器，接收http 的服务请求
# 3. 在爬虫的机器上 (scrapy) 通过requests把现在js网页的请求发送给js-server，
#   js-server通过它本机的PhantomJS下载并渲染了网页，然后把渲染好的网页源码返回给scrapy
# 4. scrapy用拿到的html源码，构建一个Response 对象，并输出
