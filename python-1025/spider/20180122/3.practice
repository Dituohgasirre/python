1. 测试字符串是否纯小写 ^[a-z]+$
2. 测试字符串是否纯大写 ^[A-Z]+$
3. 测试字符串是否纯数字 ^[0-9]+$
4. 测试字符串是否纯字母 ^[a-zA-Z]+$
5. 测试字符串是否非负整数 ^(0|[1-9][0-9]*)$
6. 测试字符串是否正整数 ^[1-9][0-9]*$
7. 测试字符串是否负整数 ^-[1-9][0-9]*$
7. 测试字符串是否整数  ^(-[1-9][0-9]*|0|[1-9][0-9]*)$
8. 测试字符串是否小数  ^(-[1-9][0-9]*|-?0|[1-9][0-9]*)\.[0-9]+$
9. 测试字符串是否只包含字母、数字、下划线，而且不以数字开头
    ^[a-zA-Z_][a-zA-Z0-9_]*$

10. 用一条sed 命令把形如2014-01-01 和2014/01/01 的日期中的减号和斜杠删除
    sed -r 's/[-/]//g'

11. 计算一个文档中每个单词的出现频率
    参考脚本 rate.sh

12. 结合shell 脚本，抽取vnc 日志中的ip 地址，端口信息，时间信息，按以下的格式列出
    Sat Jul 26 14:25:07 2014, 10.1.1.29, 44082
    参考脚本 parse_vnc_log.sh

13. 抽取dhcpd 的ip 分配记录中的ip 地址和MAC 地址，按以下的格式列出
    10.1.1.33 xx:xx:xx:xx:xx:xx
    参考脚本 parse_dhcp_log.sh

14. 抽取出page.html 中的所有iso 文件的链接的url
grep -Eo '<a href="[^"]+.iso">' page.html  | sed -r -e 's/^.{9}//' -e 's/..$//' -e 's@^@http://10.1.1.1/software/iso/redhat/@'

15. 假设文件中有以下数据
    2014-7-1
    2014-07-02
    2014-9-10
    2014-11-12
    把形如2014-7-1 的日期换成2014-07-01 的形式
    cat date | sed -r 's/-([0-9])\b/-0\1/g'

16. 写一个脚本，检查密码是否符合要求，密码的要求是最少16位，必须包含小写字母，大写字母，数字，标点符号，可以用多条正则表达式来配合检查
    参考脚本passwd_check.sh

17. 抽取出samba 的配置文件中有配置指令的行，也就是说除去注释行和空行
    sed -r -e '/^\s*[#;]/d' -e '/^\s*$/d' smb.conf


十八、用python写一个网页归档程序（爬虫），把 http://quotes.toscrape.com/ 网站上的10个页面全部下载下来。

url地址列表：
http://quotes.toscrape.com/page/1/
http://quotes.toscrape.com/page/2/
http://quotes.toscrape.com/page/3/
http://quotes.toscrape.com/page/4/
http://quotes.toscrape.com/page/5/
http://quotes.toscrape.com/page/6/
http://quotes.toscrape.com/page/7/
http://quotes.toscrape.com/page/8/
http://quotes.toscrape.com/page/9/
http://quotes.toscrape.com/page/10/

使用requests 库来获取网页

    import requests
    url = 'http://quotes.toscrape.com/page/1/'
    response = requests.get(url)
    response.status_code
    response.text


十九、用python写一个网页归档程序（爬虫），把 http://quotes.toscrape.com/ 网站上的10个页面全部下载下来。从起始地址 http://quotes.toscrape.com/ 开始，把余下的9个页面的地址找出来，然后爬取。


二十、操练数量表示法 *, +, ?, {m}, {m,n}, {m,} 的用法

1. 找出 /etc/passwd 文件中包含了 ro 或者 root 的行
2. 写一个python函数，用来检查它的第一个参数是不是纯数字
3. 写一个python函数，用来检查字符串是否符合以下要求：
    1. 以http或者https开头
    2. 尾部有或者没有斜杠
    3. 长度是20到100个字符
4. 写一个python函数，用来检查字符串是不是有效的11位数字的手机号码
5. 写一个python函数，用来检查字符串是不是符合 xxx-xxxx-xxxx 格式的手机号码



二十一、从wikipedia.org 获取Unix的解释页面，按以下要求写一个脚本

计算出该页面中出现次数最多的10个英文单词，写成一个脚本

参考：

    1. 用Firefox打开网页，手动复制文本，粘贴到一个文本文件中
       也可以用requests.get 获取网页源码，然后用re.sub删除源码中所有的标签，
       在对剩下来的文本做处理
    2. 用python的re.findall('[a-zA-Z]+', ...) 可以取出所有的“英文单词”
    3. 用python的list.sort 可以排序
    4. 用python列表的切片 (slice) 可以取出前面10个元素


示范代码：

    def count_words(words):
        res = {}
        for word in words:
            res[word] = res.get(word, 0) + 1
        return res


    def sort_words(words_count):
        return sorted(words_count.items(), key=lambda x: x[1], reverse=True)


    def show(words):
        for word, count in words:
            print('%03d %s' % (count, word))


    if __name__ == '__main__':
        n = 10
        url = 'https://en.wikipedia.org/wiki/Unix'
        r = requests.get(url)
        clean_text = re.sub(r'<[a-zA-Z0-9]+(?:\s+[^>]+)?>|</[a-zA-Z0-9]+>', '', r.text)
        words = re.findall('[A-Za-z]+', clean_text)
        words_count = count_words(words)
        sorted_words = sort_words(words_count)
        show(sorted_words[:n])


二十二、按以下要求写一个爬虫

要求：

    1. 爬网站 http://www.chinanews.com/ 首页中间大标题新闻
    2. 抓取以下内容：
        1. 新闻标题
        2. 发布时间
        3. 文章来源

工具：

    1. requests
    2. re


二十三、按以下要求写一个程序，用来获取书籍信息

1. 通过网站 http://it-ebooks-api.info/ 提供的API 查找关键字 "Python"
2. 把查找返回结果中的书籍的详细信息抓取下来，存放到json格式的文件中

技术要点：

1. 网站中"书籍详情API"不可用，需要用爬虫手动获取信息
2. 在"搜索API"的返回结果中，isbn信息可用来构建一个url，用来获取该书的详情地址，
   搜索所用的url是 http://it-ebooks-search.info/


获取书籍详情的参考步骤：

1. 通过官方API 获得书籍的基本信息 (包含了isbn)
2. 通过isbn 构建查询url，获取一个查询结果页面，类似这样：
    http://it-ebooks.info/search/?type=isbn&q=9780470259320
3. 在搜索结果页面找出书籍详情的页面url
4. 打开书籍详情页面，抓取其中的内容，需要抓取以下内容：
    Title           <-- 第一步的搜索结果中已有，不需再拿
    SubTitle        <-- 第一步的搜索结果中已有，不需再拿
    Description     <-- 第一步的搜索结果中已有，不需再拿
    Image           <-- 第一步的搜索结果中已有，不需再拿
    isbn            <-- 第一步的搜索结果中已有，不需再拿
    ID              <-- 之前所拿的是无效的，这里需要用正确的值来更新
    Publisher
    Author
    Year
    Pages
    Language
