1. 操练Series

    1. 通过python列表创建Series
        data = [1, 2, 3, 4]
        s = Series(data)
        s
    2. 通过python元组创建Series
    3. 通过numpy数组创建Series
    4. 尝试用python的集合创建Series，观察出错信息
    5. 创建Series时，提供自定义的index，尝试对这个Series做索引操作
        s = Series([1,2,3,4,5], index=['a', 'b', 'c', 'd', 'e'])
        s[0]
        s[2]
        s['a']
        s['b']
    6. 把series某个值设置为空值 (np.nan)，然后通过 s.isnull() 定位空值，并赋予新的值
        s['b'] = np.nan
        s.isnull()
        s[s.isnull()] = 101
    7. 对序列排序，按索引做反序排序，对值做排序
        s.sort_index(ascending=False)
        s.sort_values()


2. 操练DataFrame

    * 外围是列表，则列表中的每一个元素表示一行

    1. 用列表中的列表创建数据框
        DataFrame([[1,2,34],[5,6,7,8]])
        DataFrame([[1,2,34],[5,6,7]])
        DataFrame([[1,2,34],[5,6,7],[1,2]])
    2. 用列表中的字典创建数据框
        DataFrame([{'a': 1, 'b': 2}, {'c': 3, 'a': 4}])
        DataFrame([{'a': 1, 'b': 2}, {'c': 3, 'd': 4}])
    3. 用列表中的列表创建数据框，指定行和列的标签
        df = DataFrame([[1,2,3],[4,5,6]], index=['beijing','shenzhen'], columns=['a','b','c'])

    * 外围是字典，则字典中的每一个元素表示一列，外围字典的key成为列的名字

    4. 用字典中的列表创建数据框
        DataFrame({'a': [1,2,3], 'b': [2,3,4]})
    5. 用字典中的字典创建数据框
        DataFrame({'a': {'x': 1, 'y': 2}, 'b': {'u': 3, 'v': 4}})
    6. 用字典中的字典创建数据框，指定行和列的标签
         DataFrame({'a': {'x': 1, 'y': 2}, 'b': {'u': 3, 'v': 4}}, index=['u','v','x'], columns=['a','b','c'])
    7. 修改数据框的行标签和列标签，注意标签的长度应与数据形状匹配
        df.columns = ['J', 'F', 'M']
        df.index = ['upper', 'lower']
    8. 给数据框添加一个新的列
        df['K'] = 1
        df['L'] = [101,102]
        df['M'] = [101,102,103]     <-- 出错
    9. 通过两种方法取列的值
        df['J']             <-- 通用
        df.J                <-- 有条件，列的名字必须是有效的Python变量名
        df['def'] = 101     <-- 使用了一个python关键字做列名
        df['def']           <-- 正常
        df.def              <-- 出错
    10. 通过df.head, df.tail 查看数据框的头部和尾部的部分数据
        df.head()
        df.head(3)
        df.tail()
        df.tail(3)


3. 对Series做索引和切片操作
    s = Series(np.random.randint(0,10,10), index=list('abcdefghij'))
    s[0]
    s[1]
    s[-1]
    s[1:3]
    s[-3:]
    s['a']
    s['a':'d']      <-- 注意：包括了结束点


4. 对DataFrame做索引和切片操作
    df = DataFrame(np.random.randint(0,10,(7,7)), index=list('abcdefg'), columns=list('hijklmn'))
    df['h']
    df.h
    df.loc['a']                 <-- 获取行
    df.loc['c']
    df.loc['a':'c']             <-- 获取某个范围之间的行，这里使用了标签，所以包括结束点
    df.loc[:,'h']               <-- 获取某个列
    df.loc[:,'h':'j']           <-- 获取某些列
    df.loc['a':'c','h':'j']     <-- 获取某些行和列

    df.iloc[:,-3:]              <-- 取后面3列
    df.iloc[:,::2]              <-- 取偶数的列
    df.iloc[:,::-1]             <-- 取所有列，并反序排
    df.iloc[:3,:3]              <-- 取某个范围之内的行和列
    df.loc[df.index[:3],['h','j']]      <-- 混合使用标签和偏移量来做索引，这里使用了df.index来做整数到标签的切换
    df.loc[df.index[[1,3,4]], ['h','j']]    <-- 取第1,3,4行和h,j列

    df.loc['c','k']             <-- 取c行k列
    df.k.c                      <-- 简便方法，不过效率稍低


5. 对Series/DataFrame对象做reindex操作

    s.reindex(['a','c','i'])                    <-- 相当于 s[['a','c','i']]
    s['a','c','x']                              <-- 'x'不存在时可能有错，因此推荐用reindex
    s.reindex(['a','c','x'], fill_value=101)    <-- 可以填充空值

    df.reindex(['a'])                           <-- 相当于df.loc['a']，不过返回的数据类型不同
    df.reindex(index=['a'])
    df.reindex(index=['a','c'])
    df.reindex(index=['a','c'], columns=['i','k'])


6. 把阵列中大于5的值设置为0
    df = DataFrame(np.random.randint(0,10,(7,7)), index=list('abcdefg'), columns=list('hijklmn'))

    In [335]: df
    Out[335]:
       h  i  j  k  l  m
    a  0  3  0  5  0  4
    b  0  1  0  0  0  0
    c  4  2  4  0  2  5
    d  3  3  2  0  0  0
    e  5  0  1  0  2  4
    f  1  5  2  3  1  0
    g  1  0  1  0  0  5

    In [332]: df[df>5]
    Out[332]:
         h    i    j    k    l    m
    a  9.0  NaN  8.0  NaN  8.0  NaN
    b  9.0  NaN  NaN  9.0  NaN  7.0
    c  NaN  NaN  NaN  8.0  NaN  NaN
    d  NaN  NaN  NaN  6.0  NaN  8.0
    e  NaN  NaN  NaN  9.0  NaN  NaN
    f  NaN  NaN  NaN  NaN  NaN  7.0
    g  NaN  6.0  NaN  9.0  NaN  NaN

    In [333]: df[df>5] = 0

    In [334]: df
    Out[334]:
       h  i  j  k  l  m
    a  0  3  0  5  0  4
    b  0  1  0  0  0  0
    c  4  2  4  0  2  5
    d  3  3  2  0  0  0
    e  5  0  1  0  2  4
    f  1  5  2  3  1  0
    g  1  0  1  0  0  5


7. 测试Series在做算数运算时的标签自动对齐功能

    In [361]: s
    Out[361]:
    a    9
    b    3
    c    2
    d    8
    g    2
    h    2
    j    1
    dtype: int64

    In [362]: s2
    Out[362]:
    a    9
    b    3
    c    2
    d    8
    e    2
    f    2
    g    1
    dtype: int64

    In [363]: s + s2
    Out[363]:
    a    18.0
    b     6.0
    c     4.0
    d    16.0
    e     NaN
    f     NaN
    g     3.0
    h     NaN
    j     NaN
    dtype: float64

8. 构建一个DataFrame

    emp = DataFrame(np.zeros((7,4)))
    emp.columns = ['name', 'gender', 'age', 'salary']
    emp['name'] = ['Alice', 'Bob', 'Charlie', 'Jacky', 'John', 'Angle', 'David']
    emp.dtypes
    emp['gender'] = ['f','m','m','m','m','f','m']
    emp['age'] = np.random.randint(20,30,emp.index.size)
    emp['salary'] = np.random.randint(10000,20000,emp.index.size)
    emp.sort_values(['age', 'salary'])
    emp.sort_values(['age', 'salary'], ascending=[True, False])


9. 操练有重复值的index

    df = Series(np.arange(7), index=['a','a','b','c','b','d','g'])
    df.loc['a']
    df.loc[['a','b']]


10. 处理Series里面的空值

    s = Series(np.arange(7))
    s[2:6:2] = np.nan
    s.isnull()
    s[s.isnull()] = 0               <-- 直接修改原序列
    s[2:6:2] = np.nan               <-- 再设置一次空值
    s.fillna(101)                   <-- 用101代替空值
    s.fillna(method='ffill')        <-- 往前填充
        s.fillna(method='bfill')        <-- 往后填充
        s.fillna(101, inplace=True)     <-- 原地修改


11. 操练层级式索引的用法

    1. 创建一个层级式的index，并预备column的名字
        index = pd.MultiIndex.from_product([['first','second'],['chinese','math','english']])
        columns = ['Alice', 'Bob', 'Charlie', 'David', 'Fiona']

    2. 创建一个6x5的数据框，使用上面创建的index和columns
        df = DataFrame(np.random.randint(60,100,(6,5)), index=index, columns=columns)

    3. 尝试通过index来索引
        df.loc['first']
        df.loc['second']
        df.loc['second', ['Alice','Fiona']]

    4. 测试通过不同层index来对数据框排序
        df.sort_index(level=0)
        df.sort_index(level=1)

    5. 给数据框的index和columns设置名称
        df.index.names = ('semester', 'course')
        df.columns.name = 'student'

    6. 测试df.swaplevel的效果
        df.swaplevel()

    7. 测试unstack的效果，传递不同的level值
        df.unstack(level=0)
        df.unstack(level=-1)

    8. 交换index和column的位置
        df.unstack().stack(level=0)

    9. 测试sum命令与axis/level的结合使用效果
        df.sum(axis=0)
        df.sum(level=0)
        df.sum(level=1)
        df2 = df.unstack()
        df2.sum(axis=1)
        df2.sum(axis=1, level=0)
        df2.sum(axis=1, level=1)


12. 与Excel交换数据

    1. 写出到excel文件中
        emp.to_excel('/tmp/emp.xls')

    2. 从excel文件读入数据
        pd.read_excel('/tmp/emp.xlsx')


13. 从MySQL数据库读取数据

    1. 连接数据库
        from sqlalchemy import create_engine
        e = create_engine('mysql+mysqldb://lagou:abc@localhost:3306/lagou?charset=utf8', echo=False)

    2. 写入数据
        emp.to_sql(name='emp', con=e, if_exists='append', index=False)

    3. 提取数据
        df = pd.read_sql('select title,salary_low,address from store_job limit 1000', e)


14. 利用sqlite和sqlalchemy操练pandas的数据库操作

    1. 连接数据库，这里连接的是sqlite数据库，数据存放在内存中
        from sqlalchemy import create_engine
        e = create_engine('sqlite://')
        
    2. 把数据框emp的数据写到数据库中
        emp.to_sql(name='emp', con=e, if_exists='append', index=False)

    3. 从数据库中读取数据
        pd.read_sql('select * from emp', e)


15. 操练pd.merge函数

    1. 准备两份数据
        df1 = DataFrame(np.arange(12).reshape(3,4), columns=['name', 'age', 'salary', 'phone'])
        df1.name = ['Alice' ,'Bob', 'Charlie']
        df2 = DataFrame(np.arange(4).reshape(2,2), columns=['name', 'gender'])
        df2['name'] = ['Alice', 'David']
        df2['gender'] = (0,1)

    2. 合并操作
        pd.merge(df1, df2, on='name')               <-- 按照两份数据的name列的值做合并操作
        pd.merge(df1, df2, on='name', how='outer')  <-- 结果集取两个数据集key的并集
        pd.merge(df1, df2, on='name', how='left')   <-- 结果集取左边数据集的key
        pd.merge(df1, df2, on='name', how='right')  <-- 结果集取右边数据集的key

    3. 准备另外一份数据
        info = DataFrame(np.zeros((3,4)), columns=['name', 'id', 'gender', 'age'])
        info['name'] = ['Alice', 'Bob', 'Charlie']
        info['id'] = np.arange(1,4)
        info['gender'] = (0,1,1)
        info['age'] = np.random.randint(20,25,3)

        score = DataFrame(np.zeros((5,4)), columns=['id', 'semester', 'course', 'score'])
        score['id'] = np.arange(1,6)
        score['semester'] = (1,1,1,2,2)
        score['course'] = ('chinese', 'math', 'english', 'computer', 'chinese')
        score['score'] = np.random.randint(60,99,5)

    4. 按照学号合并两个数据集
        pd.merge(info, score, on='id')

    5. 修改第二个数据集的column名称
        score.columns = ['sid'] + list(score.columns)[1:]

    6. 指明left_on/right_on
        pd.merge(info, score, left_on='id', right_on='sid')

    7. 修改第一个数据集，把学号(id)设置为数据框的index
        info = info.set_index('id')

    8. 使用left_index=True来指定使用左边数据集的index
        pd.merge(info, score, left_index=True, right_on='sid')


16. 操练pd.concat函数

    1. 准备两个Series
        d1 = Series(np.arange(3))
        d2 = Series(np.arange(7,15))

    2. 测试numpy的串接效果
        np.hstack([d1.values,d2.values])

    3. pandas的串接效果
        pd.concat([d1,d2])
        pd.concat([d1,d2], axis=0)
        pd.concat([d1,d2], axis=1)

    4. 创建另外两个数据集，测试效果
        df1 = DataFrame(np.random.randint(0,99,(3,2)), columns=list('AB'))
        df2 = DataFrame(np.random.randint(0,99,(3,2)), columns=list('XY'))
        pd.concat([df1, df2], axis=1)
        pd.concat([df1, df2], axis=0)


17. 操练stack/unstack

    1. 准备数据
        df = pd.read_csv('files/tips.csv')
        d = df.groupby(['sex','smoker','day'])['tip'].mean()

    2. 测试stack/unstack
        d.unstack()
        d.unstack().unstack()
        d.unstack().unstack().stack()
        d.unstack().unstack().stack(0)      <-- 指定要堆叠的index的层次(level)


18. 操练对数据的index做修改

    1. 有数据如下
        df = DataFrame({'name': ['Alice','Bob','Charlie'],
                        'age': [82,82,36],
                        'gender': [0,1,1],
                        'score': [67,90,19]})

    2. 直接修改index或columns
        df.index = ['x','y','z']
        df.columns =  ['A', 'B', 'C']

    3. 用rename方法修改
        df.rename(columns=str.title)
        df.rename(columns=str.upper)
        df.rename(columns=lambda x: x[:2])


19. 手动计算异常值

    1. 生成一个随机样本，大小是1000个。
        s = Series(np.random.normal(0,1,1000))

    2. 计算出上下四分位
        p1 = np.percentile(s, 25)
        p3 = np.percentile(s, 75)

    3. 计算出IQR
        iqr = p3 - p1

    4. 计算出上下限
        upper = p3 + iqr * 1.5
        lower = p1 - iqr * 1.5

    5. 找出异常值
        s[(s>upper) | (s<lower)]


20. 异常值处理

    1. 创建一个200x7的随机数DataFrame
        df = DataFrame(np.random.randn(200,7))

    2. 找出样本的异常值上下限
        desc(df)

        函数desc的代码如下：

        def desc(data):
            d = data.describe()
            var = d.loc['std'] ** 2
            iqr = d.loc['75%'] - d.loc['25%']
            upper = d.loc['75%'] + 1.5 * iqr
            lower = d.loc['25%'] - 1.5 * iqr
            index = list(d.index)
            index.insert(2, 'var')
            index.extend(['iqr', 'upper', 'lower', 'outlier'])
            r = d.reindex(index)
            r.loc['var'] = var
            r.loc['iqr'] = iqr
            r.loc['upper'] = upper
            r.loc['lower'] = lower
            r.loc['outlier'] = (data > r.loc['upper']).sum() + (data < r.loc['lower']).sum()
            return r

    3. 估算一个值，用来替代异常值。注意，实际工作中，此值应结合具体业务情况确定。
        这里设定此值为 2.1。
        n = 2.1

    4. 把所有绝对值超过n的元素的值设定为此绝对值，数字的符号(sign)与原数据相同
        df[df.abs() > 2.1] = np.sign(df) * 2.1

    5. 设定一个异常值的范围，然后丢弃包含异常值的记录（行）
        upper = 2.2824261841421616
        lower = -2.1179168321658994
        b = (df > upper) | (df < lower)
        df[~b.any(axis=1)]


21. 操练Series.str的操作

    1. 加载数据

        movies = pd.read_csv('files/movies.dat', sep='::', header=None, index_col=0, nrows=10)

        将得到如下数据

        In [2410]: movies
        Out[2410]:
                                          name                        genres
        0
        1                     Toy Story (1995)   Animation|Children's|Comedy
        2                       Jumanji (1995)  Adventure|Children's|Fantasy
        3              Grumpier Old Men (1995)                Comedy|Romance
        4             Waiting to Exhale (1995)                  Comedy|Drama
        5   Father of the Bride Part II (1995)                        Comedy
        6                          Heat (1995)         Action|Crime|Thriller
        7                       Sabrina (1995)                Comedy|Romance
        8                  Tom and Huck (1995)          Adventure|Children's
        9                  Sudden Death (1995)                        Action
        10                    GoldenEye (1995)     Action|Adventure|Thriller

    2. 把genres中的第一个抽取出来

        movies.genres.str.split('|').str[0]

    3. 把genres中所有的字符转换成大写

        movies.genres.str.upper()

    4. 去genres中的后面3个字符

        movies.genres.str[-3:]

    5. 计算genres中字符串的长度

        movies.genres.str.len()


22. 操练groupby

    1. 准备数据
        df = DataFrame({'a': list('abcabcaabd'), 'b': np.random.randint(0,30,10), 'c': np.random.randint(0,30,10)})

    2. 基于a列的值分组，然后计算每组的和
        df.groupby('a').sum()

    3. 指定另外的groupby条件
        b = df.b
        b.groupby(['red', 'red', 'red', 'green', 'green', 'blue', 'blue', 'blue', 'red', 'yellow']).sum()

    4. 读取tips数据
        tips = pd.read_csv('files/tips.csv')

    5. 添加一列 tip_pct
        tips['tip_pct'] = tips.tip / tips.total_bill

    6. 按照sex和smoker列分组，并求每组tip_pct的均值，最后对值排序
        tips.groupby(['sex', 'smoker'])['tip_pct'].mean().sort_values()


23. 操练嵌套的分组

    1. 读入tips 数据
        tips = pd.read_csv('files/tips.csv')
        tips['tip_pct'] = tips.tip / tips.total_bill

    2. 创建一个函数，此函数每次会接收到一块数据，这块数据就是一个DataFrame，
        在函数内部对接收到的数据框再做分组操作。
        def f(g):
            return g.groupby('smoker')['tip_pct'].mean()

    3. 对原数据分组，然后应用上面所定义的函数
        tips.groupby('sex').apply(f)


24. 操练用百分比对数据做标准化，并打出图形

    1. 预备数据
        df = DataFrame(np.random.randint(50,size=(7,3)),
                       columns=['east', 'center', 'west'])

    2. 计算数据占一条记录中的百分比
        df2 = df.div(df.sum(axis=1), axis=0)

    3. 输出柱状图，并把柱堆叠起来
        df2.plot(kind='bar', stacked=True)


25. 操练时间序列的索引和切片操作

    1. 创建时间序列
        s = Series(np.random.randint(10,size=365), index=pd.date_range('1/1/2018', periods=365))

    2. 索引
        s['2018-01-04']
        s['1/4/2018']
        s['jan 2018']

    3. 切片
        s['2018-01-01':'2018-01-31']


26. 用read_csv导入数据，并做时间转换，指定需要转换的列

    1. 导入数据，把数据中的第0,第1两个列合并起来当成一个时间，然后转换

        df = pd.read_csv('files/ohlc.csv', sep='\s+', parse_dates=[[0,1]])

    2. 查看数据框的各列的数据类型

        df.dtypes        


27. 用自定义的频率创建一系列的时间

    idx = pd.date_range('1/1/2018', periods=365, freq='3D')
    s = Series(np.random.randint(10,size=365), index=idx)
    

28. 使用Series.shift对序列做移位操作，然后计算前后之间的百分比变化

    1. 创建数据
        s = Series(np.random.randint(10,size=10), index=pd.date_range('4/2/2018', periods=10))

    2. 计算相邻两个值的百分比改变
        s / s.shift() - 1

    3. 相当于函数Series.pct_change
        s.pct_change()


29. 通过对时间序列的index做滚动操作（滚到月末）来分组，并求每组的和

    1. 创建数据

        s = Series(np.arange(61), index=pd.date_range('/4/1/2018', periods=61))

    2. 导入相关对象，创建偏移量对象
        from pandas.tseries import offsets
        o = offsets.MonthEnd()

    3. 尝试对时间戳做滚动操作
        o.rollforward(s.index[0])
        o.rollback(s.index[-1])

    4. 分组，求和
        s.groupby(o.rollforward).sum()

        相当于以下两步： 

            factor = s.index.map(o.rollforward)
            s.groupby(factor).sum()


30. 通过对时间序列的index做滚动操作（滚到5分钟的边界）来分组，并求每组的和

    1. 创建数据，index的频率是1分钟
        s = Series(np.random.randint(1,10,60), index=pd.date_range('/4/1/2018', periods=60, freq='T'))

    2. 创建5分钟时间间距的纳秒数，因为时间戳的value属性保存的是时间的纳秒数
        ol = 5 * 60 * 10 ** 9

    3. 观察时间序列的index里面的值的纳秒数
        s.index[1].value
        s.index[-1].value
        
    4. 用pd.Timestamp 可以把一个纳秒数转换成时间戳
        new_nanosecond = s.index[0].value // ol * ol
        pd.Timestamp(new_nanosecond)

    5. 分组
        s.groupby(lambda x: pd.Timestamp(x.value // ol * ol)).sum()

        相当于： s.resample('5T').sum()


31. 操练时区的本地化和转换操作

    1. 创建一个不带时区信息的时间
        ts = pd.Timestamp('4/2/2018 10:55')

    2. 本地化
        ts2 = ts.tz_localize('US/Central')

    3. 时区转换
        ts2.tz_convert('US/Hawaii')

    4. 对比不同时区的同一个时间的值
        ts2.tz_convert('US/Hawaii').value
        ts2.tz_convert('US/Eastern').value
        ts2.tz_convert('Asia/Chongqing').value
        ts2.tz_convert('Asia/Chongqing') = ts2.tz_convert('US/Eastern')


32. 操练时段频率转换

    p = pd.Period('2018-01-01', freq='D')
    p.start_time
    p.end_time
    p.asfreq('H', how='start')
    p.asfreq('H', how='end')
    p.asfreq('T', how='end')
    p.asfreq('S', how='end')


33. 操练时间序列的重采样

    s = Series(np.random.randint(1,10,60), index=pd.date_range('/4/1/2018', periods=60, freq='T'))
    s.resample('5T')
    s.resample('5T').sum()
    s.resample('3T').sum()


34. 操练时间序列的重采样和聚合操作

    1. 从文件 files/ohlc.csv 导入数据
        df = pd.read_csv('files/ohlc.csv', sep='\s+', parse_dates=[[0,1]], index_col=0)

    2. 显示数据
        df.head()

    3. 向下重采样为5分钟，并求open/high/low/close 的均值，volume 的和

        先定义一个处理函数：

            def f(g):
                a = g[['OPEN','HIGH','LOW','CLOSE']].mean()
                b = g.VOLUME.sum()
                return Series(a.tolist() + [b], index=g.columns)

        重采样，计算相关统计量：

            df.resample('5T').agg(f)
