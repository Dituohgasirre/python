** pandas 初步

# 把Python 的list 对象和dict 对象转成Series 对象，并加上自定义的索引
# 把Series 对象里面缺失(missing) 的元素找出来，赋一个指定的值
# 分別取出Series 对象的index 和值 (s.index, s.values)
# 从一个list of lists 或者list of dicts 创建出一个DataFrame 对象，并指定columns 和index
# 从一个dict of lists 或者dict of dicts 创建出一个DataFrame 对象，并指定columns 和index
# 对一个DataFrame 的一个column 进行赋值 (with value of type scalar, list, np.array, Series)
# Index 对象是只读的，类似阵列又类似不会改变的集合
# 查看Series/DataFrame前面或后面的部分内容 (head/tail)
# 对Series/DataFrame做索引/切片操作 (标签名称，偏移位置)
# Series 对象做按照label slicing 的时候与Python slicing 不同（包含了结束点）
# 增加/删除DataFrame的列 (df['name'] = value, del df['name'])
# 使用reindex 重建Series 对象
# 使用reindex 重建DataFrame 对象
# 丟弃某个轴上的元素 (s.drop, df.drop)
# 取出DataFrame 对象里面的某些行  （loc, iloc）
# 取出DataFrame 对象里面的某些列  （loc, iloc）
# 取出DataFrame 对象里面的某些行和列 (loc, iloc)
# 按照某列的值过滤DataFrame 对象里面的行
# 按照DataFrame的值过滤DataFrame 对象里面所有的元素
# 对DataFrame 做数学运算
# DataFrame 与Series 之间的操作 （+, -, DataFrame.add, DataFrame.sub)
# 使用DataFrame.apply, DataFrame.applymap (apply: Axis-wise, applymap: Element-wise)
# 使用Series.map 来操作Series 里面的数据 (Series.map is like DataFrame.applymap)
# 按照index 来sort 一个Series (Series.sort_index)
# 按照values 来sort 一个Series (Series.sort_values)
# 按照行的index 或者列的index 来sort 一个DataFrame (sort_index(), sort_index(axis=1))
# 按照多列的值来对DataFrame 的行排序 (frame.sort_values(by=['a', 'b']))
# 按照行的值来对DataFrame 的列排序 (axis=1)
# Series 和DataFrame 的index 中出现同名元素的现象，及按这些同名的元素取值时的特点
# 常见的DataFrame reduction 函数及其使用方法 (sum, mean, max, ...)
# Series/DataFrame的describe方法的用法
# 计算Series 中的unique values, value counts (s.unique(), s.value_counts)
# 用isin 方法来过滤Series 中的数据 (s.isin(['a', 'b', 'c']))
# 用isin 方法来取出Series 中的一个子集 (s[s.isin(['b', 'c']))
# 计算DataFrame 中每个column 的 value counts (DataFrame.apply with Series.value_counts method)
# 使用Series 的dropna, fillna, isnull, notnull 方法来过滤NA 的值
# 使用DataFrame 的dropna 来删除一些行或者列
# 使用Series 和DataFrame 的fillna 来补充缺失的值
# 层级式索引 (hierarchical indexing, of Series and DataFrame)
# 把层级式索引中的外层索引和内层索引互换(swaplevel)，分別排序(sortlevel)
# 按照层级索引对数据做sum 操作
# 把DataFrame 的列中的数据拿来做它的index (set_index)
# 把DataFrame 的index 拿来创建新的列，或恢复默认的index (reset_index)


** 数据加载，存储，文件格式

# 使用pd.read_csv/pd.read_table 来读取文本文件
# 指定pd.read_csv 结果中列名字，行的名字 (header, index_col, names)
# 指定pd.read_csv 使用正则表达式做分隔符 (sep)
# 为pd.read_csv 定义缺失值 (na_values)
# 使pd.read_csv 忽略文件中的某些行，忽略头部或尾部某些行 (skiprows, skipfooter)
# 使pd.read_csv 只读取指定数量的行 (nrows)
# 用pd.read_csv 分段处理大文件 (chunksize, get_chunk)
# 用pd.to_csv 把数据保存到外部文件中
# 使pd.to_csv 不输出列的名字和行的名字 (header, index)
# 使pd.to_csv 只保存某些列，且调整列的顺序 (columns)
# 使用HDF5 来保存大量的数据集 (pd.HDFStore)
# 在使用HDF5 时启用压缩 (complevel, complib)
# 从MS excel 文件中加载数据，把数据写入MS excel 文件
# 把数据存入关系型数据库，从数据库中读取数据


** 数据清洗，数据转换

# 用pd.merge 来合并两个数据集的行 (inner, outer, left, right, default keys, single key, multiple keys)
# DataFrame.join 的特点及应用场合 (left join on index, convenient for merging many data sets)
# pd.concat 按列的index 或者按行的index 串接多个数据集
# pd.concat 与pd.merge 的异同点（一个是串接，一个是合并）
# 用DataFrame.stack, DataFrame.unstack 对数据集做形状转换
# 用DataFrame.pivot 方法转换数据的形状 (pivoting 就是一种reshaping)
# 用 DataFrame.combine_first, DataFrame.update 填补数据集中的空缺
# 列出数据集中的重复条目 (duplicated, s[s.duplicated()]用于取出重复的值)
# 去除数据集中的重复条目 (drop_duplicates)
# 用Series.map 对数据做转换 (dict, series, function)
# 用Series, DataFrame 的replace 方法来对数据集中的某些数据做替换
# 替换整个轴的标签 (index=..., columns=...)
# 对轴的标签做修改 (rename, with function, or dict)
# 使用pd.cut 和pd.qcut 对数据集做分割 (labels)
# 发现数据集中的异常值，并定位，修改 (df.abs, df.any, np.sign)
# 使用np.random.permutation, np.random.randint 做随机取样和置换
# 生成一个阵列的dummy/indicator
# 使用Series.str 下面常用的文本处理方法来处理Series 实例对象 (vecterize)


** 数据聚合(aggregation) 操作和组操作(groupby)

# “组聚合”的原理 (split->apply->combine)
# 对Series, DataFrame 做分组操作 (the use of key(s))
# 遍历分组对象，获取每一个组的key 和数据
# 把数据集按照key 生成dict
# 在0轴分组，在1轴分组 (axis=0, axis=1)
# 对分组中的某些列做聚合操作，而不是全部的列 ([name], [[name1,name2]])
# 用dict 或者Series 实例作为key 进行分组
# 用函数作为key 进行分组
# 用index 的level 作为key 进行分组 (level=...)
# 使用常见的聚合函数mean, min, max, sum, ... (g.mean)
# 使用agg 方法给分组应用聚合函数/方法 (grouped.agg, g.agg('mean'), g.agg(np.mean))
# 给分组应用多个聚合函数，并能定义结果中列的名字 ( g.agg([('name1',func1),('name2',func2)]) )
# 对DataFrame 分组指定的列应用指定的聚合函数
# 使得分组所用的key 不成为聚合结果中的index (as_index=False, or reset_index)
# transform 函数的原理
# 使用transform 方法给分组应用转换函数/方法 (grouped.transform)
# 组操作的分类 （聚合agg，转换transform，其它apply）
# 使用apply 方法给分组应用各种处理函数/方法 (grouped.apply)
# 利用pd.cut, pd.qcut 的结果作为分组的key，做bucket analysis
# 在做填补缺失值操作时针对不同的分组使用不同的值 (use mean, or specified by a dict)
# 在分组的处理函数中再做分组操作，即嵌套式的数据分组处理
# 通过计算百分比的方式来常规化(normalize)数据 (df.div)


** 时间序列

# 创建任意时间的datetime 实例对象 (datetime(year, month, day, ...))
# 把datetime 对象转换成str 对象 (str, dt.strftime)
# 把str 转换成datetime 对象 (datetime.strptime, dateutil.parser.parse)
# 使用pd.to_datetime 来把日期时间字符串转换成日期时间
# 创建时间序列 (convert a list)
# 对时间序列Series, DataFrame 做按索引定位，切片
# 检测出序列是否有重复的index，并能去除重复 (s.index.unique, s.groupby)
# 创建时间范围 (pd.date_range, pd.DatetimeIndex, pd.to_datetime)
# 按照常见的频率创建时间范围 (freq, pd.tseries.offsets)
# 对时间序列的值做前移位和后移位 (Series.shift)
# 使用移位的方法计算时间序列中前后值的百分比变化
# 给日期做偏移操作 (rollforward, rollback, ...)
# 利用对日期的偏移做分组操作 (g = s.groupby(o.rollforward))
# 对Timestamp 和时间序列做时区本地化和时区转换 (Timestamp.tz_localize, Timestamp.tz_convert)
# 不同时区的时间值之间的运算的特点（ts.value, 结果为UTC）
# 创建时段 (pd.Period)
# 对时段做算数运算 (+/-)
# 创建时段范围 (pd.period_range, pd.PeriodIndex)
# 对时段做频率转换 (asfreq, how)
# 找出时段的首和尾 (p.start_time, p.end_time)
# 把时段转成时间戳 (to_timestamp)
# 使用resample 来做重采样及聚合操作
# OHLC 聚合方法 (agg(f) + unstack)
# 使用groupby 来做重采样 (pd.to_datetime(s.index.astype(int) // (ns5min) * ns5min))
# 向上重采样，插值的行为，同级重采样的特点
# 用时段period 来做重采样，对源频率和目标频率的要求
# 对时间序列数据绘图 (plot stock_px.csv)
